{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tables\n",
    "\n",
    "from tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = 'tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring a Column Descriptor\n",
    "Now, imagine that we have a particle detector and we want to create a table object in order to save data retrieved from it. You need first to define the table, the number of columns it has, what kind of object is contained in each column, and so on.\n",
    "\n",
    "Our particle detector has a TDC (Time to Digital Converter) counter with a dynamic range of 8 bits and an ADC (Analogical to Digital Converter) with a range of 16 bits. For these values, we will define 2 fields in our record object called TDCcount and ADCcount. We also want to save the grid position in which the particle has been detected, so we will add two new fields called grid_i and grid_j. Our instrumentation also can obtain the pressure and energy of the particle. The resolution of the pressure-gauge allows us to use a single-precision float to store pressure readings, while the energy value will need a double-precision float. Finally, to track the particle we want to assign it a name to identify the kind of the particle it is and a unique numeric identifier. So we will add two more fields: name will be a string of up to 16 characters, and idnumber will be an integer of 64 bits (to allow us to store records for extremely large numbers of particles).\n",
    "\n",
    "Having determined our columns and their types, we can now declare a new Particle class that will contain all this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Particle(IsDescription):\n",
    "    name      = StringCol(16)   # 16-character String\n",
    "    idnumber  = Int64Col()      # Signed 64-bit integer\n",
    "    ADCcount  = UInt16Col()     # Unsigned short integer\n",
    "    TDCcount  = UInt8Col()      # unsigned byte\n",
    "    grid_i    = Int32Col()      # 32-bit integer\n",
    "    grid_j    = Int32Col()      # 32-bit integer\n",
    "    pressure  = Float32Col()    # float  (single-precision)\n",
    "    energy    = Float64Col()    # double (double-precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GabesRandomGroup(IsDescription):\n",
    "    name        = StringCol(16)\n",
    "    array       = Int8Col()\n",
    "    atomic_int  = Int8Col()\n",
    "    regular_int = Int8Col()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definition class is self-explanatory. Basically, you declare a class variable for each field you need. As its value you assign an instance of the appropriate Col subclass, according to the kind of column defined (the data type, the length, the shape, etc). See the The Col class and its descendants for a complete description of these subclasses. See also Supported data types in PyTables for a list of data types supported by the Col constructor.\n",
    "\n",
    "From now on, we can use Particle instances as a descriptor for our detector data table. We will see later on how to pass this object to construct the table. But first, we must create a file where all the actual data pushed into our table will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a PyTables file from scratch\n",
    "Use the top-level **`open_file()`** function to create a PyTables file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File(filename=tmp/tutorial1.h5, title='Test file', mode='w', root_uep='/', filters=Filters(complevel=0, shuffle=False, bitshuffle=False, fletcher32=False, least_significant_digit=None))\n",
       "/ (RootGroup) 'Test file'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file = open_file(\n",
    "    os.path.join(tmp_path, 'tutorial1.h5'), \n",
    "    mode=\"w\", \n",
    "    title=\"Test file\",\n",
    "    root_uep=\"/\"\n",
    ")\n",
    "\n",
    "h5file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`open_file()`** is one of the objects imported by the **`from tables import *`** statement. Here, we are saying that we want to create a new file in the current working directory called “tutorial1.h5” in “w”rite mode and with an descriptive title string (“Test file”). This function attempts to open the file, and if successful, returns the File (see The File Class) object instance h5file. The root of the object tree is specified in the instance’s root attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new group\n",
    "Now, to better organize our data, we will create a group called detector that branches from the root node. We will save our particle data table in this group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/detector (Group) 'Detector information'\n",
       "  children := []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = h5file.create_group(\n",
    "    where=\"/\",\n",
    "    name='detector',\n",
    "    title='Detector information',\n",
    "    filters=None,\n",
    "    createparents=False)\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/gabes_group (Group) 'gabes random group'\n",
       "  children := []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gabes_group = h5file.create_group(\n",
    "    where='/',\n",
    "    name='gabes_group',\n",
    "    title='gabes random group')\n",
    "gabes_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have taken the File instance h5file and invoked its File.create_group() method to create a new group called detector branching from “/” (another way to refer to the h5file.root object we mentioned above). This will create a new Group (see The Group class) object instance that will be assigned to the variable group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new table\n",
    "Let’s now create a Table (see The **[Table class](https://www.pytables.org/usersguide/libref/structured_storage.html#tableclassdescr)**) object as a branch off the newly-created group. We do that by calling the **`File.create_table()`** method of the h5file object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/detector/readout (Table(0,)) 'Readout example'\n",
       "  description := {\n",
       "  \"ADCcount\": UInt16Col(shape=(), dflt=0, pos=0),\n",
       "  \"TDCcount\": UInt8Col(shape=(), dflt=0, pos=1),\n",
       "  \"energy\": Float64Col(shape=(), dflt=0.0, pos=2),\n",
       "  \"grid_i\": Int32Col(shape=(), dflt=0, pos=3),\n",
       "  \"grid_j\": Int32Col(shape=(), dflt=0, pos=4),\n",
       "  \"idnumber\": Int64Col(shape=(), dflt=0, pos=5),\n",
       "  \"name\": StringCol(itemsize=16, shape=(), dflt=b'', pos=6),\n",
       "  \"pressure\": Float32Col(shape=(), dflt=0.0, pos=7)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1394,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = h5file.create_table(\n",
    "    where=group,\n",
    "    name='readout',\n",
    "    description=Particle,\n",
    "    title=\"Readout example\",\n",
    "    filters=None,\n",
    "    expectedrows=10000,\n",
    "    chunkshape=None,\n",
    "    byteorder=None,\n",
    "    createparents=False,\n",
    "    obj=None,\n",
    "    track_times=True,\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/gabes_group/gabes_table (Table(0,)) 'Gabe Table Example'\n",
       "  description := {\n",
       "  \"array\": Int8Col(shape=(), dflt=0, pos=0),\n",
       "  \"atomic_int\": Int8Col(shape=(), dflt=0, pos=1),\n",
       "  \"name\": StringCol(itemsize=16, shape=(), dflt=b'', pos=2),\n",
       "  \"regular_int\": Int8Col(shape=(), dflt=0, pos=3)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (3449,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gabes_table = h5file.create_table(\n",
    "    where=gabes_group,\n",
    "    name='gabes_table',\n",
    "    description=GabesRandomGroup,\n",
    "    title=\"Gabe Table Example\",\n",
    "    filters=None,\n",
    "    expectedrows=10000,\n",
    "    chunkshape=None,\n",
    "    byteorder=None,\n",
    "    createparents=False,\n",
    "    obj=None,\n",
    "    track_times=True,\n",
    ")\n",
    "gabes_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the Table instance under group. We assign this table the node name “readout”. The Particle class declared before is the description parameter (to define the columns of the table) and finally we set “Readout example” as the Table title. With all this information, a new Table instance is created and assigned to the variable table.\n",
    "\n",
    "If you are curious about how the object tree looks right now, simply print the File instance variable h5file, and examine the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tutorial1.h5 (File) 'Test file'\n",
      "Last modif.: 'Mon Sep 23 10:38:05 2019'\n",
      "Object Tree: \n",
      "/ (RootGroup) 'Test file'\n",
      "/detector (Group) 'Detector information'\n",
      "/detector/readout (Table(0,)) 'Readout example'\n",
      "/gabes_group (Group) 'gabes random group'\n",
      "/gabes_group/gabes_table (Table(0,)) 'Gabe Table Example'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(h5file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a dump of the object tree is displayed. It’s easy to see the Group and Table objects we have just created. If you want more information, just type the variable containing the File instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tutorial1.h5 (File) 'Test file'\n",
      "Last modif.: 'Mon Sep 23 10:38:05 2019'\n",
      "Object Tree: \n",
      "/ (RootGroup) 'Test file'\n",
      "/detector (Group) 'Detector information'\n",
      "/detector/readout (Table(0,)) 'Readout example'\n",
      "/gabes_group (Group) 'gabes random group'\n",
      "/gabes_group/gabes_table (Table(0,)) 'Gabe Table Example'\n",
      "\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "File(filename=tmp/tutorial1.h5, title='Test file', mode='w', root_uep='/', filters=Filters(complevel=0, shuffle=False, bitshuffle=False, fletcher32=False, least_significant_digit=None))\n",
       "/ (RootGroup) 'Test file'\n",
       "/detector (Group) 'Detector information'\n",
       "/detector/readout (Table(0,)) 'Readout example'\n",
       "  description := {\n",
       "  \"ADCcount\": UInt16Col(shape=(), dflt=0, pos=0),\n",
       "  \"TDCcount\": UInt8Col(shape=(), dflt=0, pos=1),\n",
       "  \"energy\": Float64Col(shape=(), dflt=0.0, pos=2),\n",
       "  \"grid_i\": Int32Col(shape=(), dflt=0, pos=3),\n",
       "  \"grid_j\": Int32Col(shape=(), dflt=0, pos=4),\n",
       "  \"idnumber\": Int64Col(shape=(), dflt=0, pos=5),\n",
       "  \"name\": StringCol(itemsize=16, shape=(), dflt=b'', pos=6),\n",
       "  \"pressure\": Float32Col(shape=(), dflt=0.0, pos=7)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (1394,)\n",
       "/gabes_group (Group) 'gabes random group'\n",
       "/gabes_group/gabes_table (Table(0,)) 'Gabe Table Example'\n",
       "  description := {\n",
       "  \"array\": Int8Col(shape=(), dflt=0, pos=0),\n",
       "  \"atomic_int\": Int8Col(shape=(), dflt=0, pos=1),\n",
       "  \"name\": StringCol(itemsize=16, shape=(), dflt=b'', pos=2),\n",
       "  \"regular_int\": Int8Col(shape=(), dflt=0, pos=3)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (3449,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h5file)\n",
    "print('---------------------------------------')\n",
    "h5file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/detector (Group) 'Detector information'\n",
       "  children := ['readout' (Table)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/gabes_group (Group) 'gabes random group'\n",
       "  children := ['gabes_table' (Table)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gabes_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed information is displayed about each object in the tree. Note how Particle, our table descriptor class, is printed as part of the readout table description information. In general, you can obtain much more information about the objects and their children by just printing them. That introspection capability is very useful, and I recommend that you use it extensively.\n",
    "\n",
    "The time has come to fill this table with some values. First we will get a pointer to the Row (see The Row class) instance of this table instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/detector/readout.row (Row), pointing to row #0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particle = table.row\n",
    "particle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row attribute of table points to the Row instance that will be used to write data rows into the table. We write data simply by assigning the Row instance the values for each row as if it were a dictionary (although it is actually an extension class), using the column names as keys.\n",
    "\n",
    "Below is an example of how to write rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    particle['name']  = 'Particle: %6d' % (i)\n",
    "    particle['TDCcount'] = i % 256\n",
    "    particle['ADCcount'] = (i * 256) % (1 << 16)\n",
    "    particle['grid_i'] = i\n",
    "    particle['grid_j'] = 10 - i\n",
    "    particle['pressure'] = float(i*i)\n",
    "    particle['energy'] = float(particle['pressure'] ** 4)\n",
    "    particle['idnumber'] = i * (2 ** 34)\n",
    "    # Insert a new particle record\n",
    "    particle.append()\n",
    "\n",
    "# Flush the table buffer\n",
    "table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s the same method we used to fill a new table. PyTables knows that this table is on disk, and when you add new records, they are appended to the end of the table.\n",
    "\n",
    "If you look carefully at the code you will see that we have used the table.row attribute to create a table row and fill it with the new values. Each time that its append() method is called, the actual row is committed to the output buffer and the row pointer is incremented to point to the next table record. When the buffer is full, the data is saved on disk, and the buffer is reused again for the next cycle.\n",
    "\n",
    "Caveat emptor: Do not forget to always call the flush() method after a write operation, or else your tables will not be updated!\n",
    "\n",
    "Let’s have a look at some rows in the modified table and verify that our new data has been appended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               name      pressure        energy    grid_i   grid_j    TDCcount\n",
      "b'Particle:      0' |         0.0 |           0 |      0 |     10 |        0 \\|\n",
      "b'Particle:      1' |         1.0 |           1 |      1 |      9 |        1 \\|\n",
      "b'Particle:      2' |         4.0 |         256 |      2 |      8 |        2 \\|\n",
      "b'Particle:      3' |         9.0 |        6561 |      3 |      7 |        3 \\|\n",
      "b'Particle:      4' |        16.0 |   6.554e+04 |      4 |      6 |        4 \\|\n",
      "b'Particle:      5' |        25.0 |   3.906e+05 |      5 |      5 |        5 \\|\n",
      "b'Particle:      6' |        36.0 |    1.68e+06 |      6 |      4 |        6 \\|\n",
      "b'Particle:      7' |        49.0 |   5.765e+06 |      7 |      3 |        7 \\|\n",
      "b'Particle:      8' |        64.0 |   1.678e+07 |      8 |      2 |        8 \\|\n",
      "b'Particle:      9' |        81.0 |   4.305e+07 |      9 |      1 |        9 \\|\n"
     ]
    }
   ],
   "source": [
    "print('%19s'%table.colnames[6], end='')\n",
    "print('%14s'%table.colnames[7], end='')\n",
    "print('%14s'%table.colnames[2], end='')\n",
    "print('%10s'%table.colnames[3], end='')\n",
    "print('%9s'%table.colnames[4], end='')\n",
    "print('%12s'%table.colnames[1], end='\\n')\n",
    "    \n",
    "for r in table.iterrows():\n",
    "    print(\"%-16s | %11.1f | %11.4g | %6d | %6d | %8d \\|\" % \n",
    "          (r['name'], r['pressure'], r['energy'], \n",
    "           r['grid_i'], r['grid_j'], r['TDCcount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading (and selecting) data in a table\n",
    "Ok. We have our data on disk, and now we need to access it and select from specific columns the values we are interested in. See the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25.0, 36.0, 49.0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = h5file.root.detector.readout\n",
    "pressure = [x['pressure'] for x in table.iterrows() if x['TDCcount'] > 3 and 20 <= x['pressure'] < 50]\n",
    "pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates a “shortcut” to the readout table deeper on the object tree. As you can see, we use the natural naming schema to access it. We also could have used the h5file.get_node() method, as we will do later on.\n",
    "You will recognize the last two lines as a Python list comprehension. It loops over the rows in table as they are provided by the Table.iterrows() iterator. The iterator returns values until all the data in table is exhausted. These rows are filtered using the expression:\n",
    "\n",
    "```python \n",
    "['TDCcount'] > 3 and 20 <= x['pressure'] < 50\n",
    "```\n",
    "\n",
    "So, we are selecting the values of the pressure column from filtered records to create the final list and assign it to pressure variable.\n",
    "We could have used a normal for loop to accomplish the same purpose, but I find comprehension syntax to be more compact and elegant.\n",
    "PyTables do offer other, more powerful ways of performing selections which may be more suitable if you have very large tables or if you need very high query speeds. They are called in-kernel and indexed queries, and you can use them through Table.where() and other related methods.\n",
    "Let’s use an in-kernel selection to query the name column for the same set of cuts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Particle:      5', b'Particle:      6', b'Particle:      7']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [ x['name'] for x in table.where(\"\"\"(TDCcount > 3) & (20 <= pressure) & (pressure < 50)\"\"\") ]\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-kernel and indexed queries are not only much faster, but as you can see, they also look more compact, and are among the greatests features for PyTables, so be sure that you use them a lot. See [Condition Syntax](https://www.pytables.org/usersguide/condition_syntax.html#condition-syntax) and [Accelerating your searches](https://www.pytables.org/usersguide/optimization.html#searchoptim) for more information on in-kernel and indexed selections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "A special care should be taken when the query condition includes string literals. Indeed Python 2 string literals are string of bytes while Python 3 strings are unicode objects.\n",
    "With reference to the above definition of Particle it has to be noted that the type of the “name” column do not change depending on the Python version used (of course). It always corresponds to strings of bytes.\n",
    "Any condition involving the “name” column should be written using the appropriate type for string literals in order to avoid TypeErrors.\n",
    "Suppose one wants to get rows corresponding to specific particle names.\n",
    "The code below will work fine in Python 2 but will fail with a TypeError in Python 3:\n",
    "\n",
    "```python\n",
    "condition = '(name == \"Particle:      5\") | (name == \"Particle:      7\")'\n",
    "for record in table.where(condition):  # TypeError in Python3\n",
    "...     # do something with \"record\"\n",
    "```\n",
    "The reason is that in Python 3 “condition” implies a comparison between a string of bytes (“name” column contents) and an unicode literals.\n",
    "The correct way to write the condition is:\n",
    "```python\n",
    "condition = '(name == b\"Particle:      5\") | (name == b\"Particle:      7\")'\n",
    "```\n",
    "That’s enough about selections for now. The next section will show you how to save these selected results to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new array objects\n",
    "In order to separate the selected data from the mass of detector data, we will create a new group columns branching off the root group. Afterwards, under this group, we will create two arrays that will contain the selected data. First, we create the group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcolumns = h5file.create_group(h5file.root, \"columns\", \"Pressure and Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time we have specified the first parameter using natural naming (h5file.root) instead of with an absolute path string (“/”).\n",
    "Now, create the first of the two Array objects we’ve just mentioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/columns/pressure (Array(3,)) 'Pressure column selection'\n",
       "  atom := Float64Atom(shape=(), dflt=0.0)\n",
       "  maindim := 0\n",
       "  flavor := 'python'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file.create_array(where=gcolumns, \n",
    "                    name='pressure', \n",
    "                    obj=pressure, \n",
    "                    title=\"Pressure column selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know the first two parameters of the File.create_array() methods (these are the same as the first two in create_table): they are the parent group where Array will be created and the Array instance name. The third parameter is the object we want to save to disk. In this case, it is a NumPy array that is built from the selection list we created before. The fourth parameter is the title.\n",
    "Now, we will save the second array. It contains the list of strings we selected before: we save this object as-is, with no further conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/columns/name (Array(3,)) 'Name column selection'\n",
       "  atom := StringAtom(itemsize=16, shape=(), dflt=b'')\n",
       "  maindim := 0\n",
       "  flavor := 'python'\n",
       "  byteorder := 'irrelevant'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file.create_array(gcolumns, 'name', names, \"Name column selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `File.create_array()` accepts names (which is a regular Python list) as an object parameter. Actually, it accepts a variety of different regular objects (see create_array()) as parameters. The flavor attribute (see the output above) saves the original kind of object that was saved. Based on this flavor, PyTables will be able to retrieve exactly the same object from disk later on.\n",
    "\n",
    "Note that in these examples, the `create_array` method returns an Array instance that is not assigned to any variable. Don’t worry, this is intentional to show the kind of object we have created by displaying its representation. The Array objects have been attached to the object tree and saved to disk, as you can see if you print the complete object tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tutorial1.h5 (File) 'Test file'\n",
      "Last modif.: 'Mon Sep 23 10:38:09 2019'\n",
      "Object Tree: \n",
      "/ (RootGroup) 'Test file'\n",
      "/columns (Group) 'Pressure and Name'\n",
      "/columns/name (Array(3,)) 'Name column selection'\n",
      "/columns/pressure (Array(3,)) 'Pressure column selection'\n",
      "/detector (Group) 'Detector information'\n",
      "/detector/readout (Table(10,)) 'Readout example'\n",
      "/gabes_group (Group) 'gabes random group'\n",
      "/gabes_group/gabes_table (Table(0,)) 'Gabe Table Example'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(h5file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing the file and looking at its content\n",
    "To finish this first tutorial, we use the close method of the h5file File object to close the file before exiting Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now created your first PyTables file with a table and two arrays. You can examine it with any generic HDF5 tool, such as h5dump or h5ls. Here is what the tutorial1.h5 looks like when read with the h5ls program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/                        Group\r\n",
      "/columns                 Group\r\n",
      "/columns/name            Dataset {3}\r\n",
      "    Data:\r\n",
      "        (0) \"Particle:      5\", \"Particle:      6\", \"Particle:      7\"\r\n",
      "/columns/pressure        Dataset {3}\r\n",
      "    Data:\r\n",
      "        (0) 25, 36, 49\r\n",
      "/detector                Group\r\n",
      "/detector/readout        Dataset {10/Inf}\r\n",
      "    Data:\r\n",
      "        (0) {0, 0, 0, 0, 10, 0, \"Particle:      0\", 0},\r\n",
      "        (1) {256, 1, 1, 1, 9, 17179869184, \"Particle:      1\", 1},\r\n",
      "        (2) {512, 2, 256, 2, 8, 34359738368, \"Particle:      2\", 4},\r\n",
      "        (3) {768, 3, 6561, 3, 7, 51539607552, \"Particle:      3\", 9},\r\n",
      "        (4) {1024, 4, 65536, 4, 6, 68719476736, \"Particle:      4\", 16},\r\n",
      "        (5) {1280, 5, 390625, 5, 5, 85899345920, \"Particle:      5\", 25},\r\n",
      "        (6) {1536, 6, 1679616, 6, 4, 103079215104, \"Particle:      6\", 36},\r\n",
      "        (7) {1792, 7, 5764801, 7, 3, 120259084288, \"Particle:      7\", 49},\r\n",
      "        (8) {2048, 8, 16777216, 8, 2, 137438953472, \"Particle:      8\", 64},\r\n",
      "        (9) {2304, 9, 43046721, 9, 1, 154618822656, \"Particle:      9\", 81}\r\n",
      "/gabes_group             Group\r\n",
      "/gabes_group/gabes_table Dataset {0/Inf}\r\n",
      "    Data:\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! h5ls -rd tmp/tutorial1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the output as displayed by the “ptdump” PyTables utility (located in utils/ directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) 'Test file'\r\n",
      "/columns (Group) 'Pressure and Name'\r\n",
      "/columns/name (Array(3,)) 'Name column selection'\r\n",
      "/columns/pressure (Array(3,)) 'Pressure column selection'\r\n",
      "/detector (Group) 'Detector information'\r\n",
      "/detector/readout (Table(10,)) 'Readout example'\r\n",
      "/gabes_group (Group) 'gabes random group'\r\n",
      "/gabes_group/gabes_table (Table(0,)) 'Gabe Table Example'\r\n"
     ]
    }
   ],
   "source": [
    "! ptdump tmp/tutorial1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass the -v or -d options to ptdump if you want more verbosity. Try them out!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
